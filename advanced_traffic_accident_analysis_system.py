# -*- coding: utf-8 -*-
"""Advanced Traffic Accident Analysis System

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jqWHYDb9D97MVpCxlTe9z-_Zq-v_rMVX
"""

# =============================================================================
# ADVANCED TRAFFIC ACCIDENT ANALYSIS SYSTEM
# Professional Data Science Pipeline with Interactive Visualizations
# Dataset: US Accidents Dataset (Kaggle)
# Task 4 - Skillcraft Technology Internship
# =============================================================================

# Installation (Run once in Colab/Jupyter)
# !pip install -q plotly pandas numpy seaborn scikit-learn statsmodels folium

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import silhouette_score
import statsmodels.api as sm
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
import warnings
warnings.filterwarnings('ignore')

# PROFESSIONAL NEUTRAL COLOR PALETTE
COLORS = {
    'bg_primary': '#0A0E1A',
    'bg_secondary': '#151925',
    'surface': '#1E2430',
    'surface_light': '#2A3341',
    'accent_blue': '#5B9BD5',
    'accent_cyan': '#70C9C9',
    'accent_teal': '#5EEAD4',
    'accent_slate': '#94A3B8',
    'accent_steel': '#64748B',
    'text_primary': '#F1F5F9',
    'text_secondary': '#94A3AF',
    'grid': '#2D3748',
    'border': '#475569',
    'success': '#34D399',
    'warning': '#FBBF24',
    'danger': '#818CF8'
}

print("=" * 80)
print("ADVANCED TRAFFIC ACCIDENT ANALYSIS SYSTEM")
print("=" * 80)

# =============================================================================
# STEP 1: DATA COLLECTION & LOADING
# =============================================================================
def load_accident_data():
    """Load US Accidents dataset with intelligent sampling"""
    print("\n[1/6] LOADING DATASET...")

    # Sample data URL (replace with actual dataset path)
    # For demo: Generate synthetic data similar to US Accidents structure

    try:
        # Attempt to load from actual source
        # url = 'path_to_us_accidents.csv'
        # df = pd.read_csv(url, nrows=50000)

        # Generate sample data for demonstration
        df = generate_sample_accident_data(50000)

        print(f"‚úì Dataset loaded successfully: {df.shape[0]:,} rows √ó {df.shape[1]} columns")
        print(f"\nüìä Dataset Overview:")
        print(f"   ‚Ä¢ Time Range: {df['start_time'].min()} to {df['start_time'].max()}")
        print(f"   ‚Ä¢ States Covered: {df['state'].nunique()}")
        print(f"   ‚Ä¢ Severity Levels: {sorted(df['severity'].unique())}")

        return df
    except Exception as e:
        print(f"‚úó Error: {e}")
        return generate_sample_accident_data(50000)

def generate_sample_accident_data(n_samples=50000):
    """Generate realistic sample accident data"""
    np.random.seed(42)

    # Geographic data (US coordinates)
    latitudes = np.random.uniform(25.0, 49.0, n_samples)  # Continental US
    longitudes = np.random.uniform(-125.0, -65.0, n_samples)

    # Temporal data
    start_dates = pd.date_range('2020-01-01', '2023-12-31', periods=n_samples)
    hours = np.random.randint(0, 24, n_samples)

    # Severity (1-4, with realistic distribution)
    severity = np.random.choice([1, 2, 3, 4], n_samples, p=[0.15, 0.45, 0.30, 0.10])

    # Weather conditions
    weather_conditions = ['Clear', 'Cloudy', 'Rain', 'Snow', 'Fog', 'Heavy Rain', 'Light Rain']
    weather = np.random.choice(weather_conditions, n_samples, p=[0.35, 0.25, 0.15, 0.08, 0.07, 0.05, 0.05])

    # Temperature (Fahrenheit)
    temperature = np.random.normal(65, 20, n_samples)

    # Wind speed (mph)
    wind_speed = np.abs(np.random.normal(8, 5, n_samples))

    # Road features
    bump = np.random.choice([True, False], n_samples, p=[0.15, 0.85])
    crossing = np.random.choice([True, False], n_samples, p=[0.25, 0.75])
    junction = np.random.choice([True, False], n_samples, p=[0.20, 0.80])
    traffic_signal = np.random.choice([True, False], n_samples, p=[0.30, 0.70])

    # Time of day
    sunrise_sunset = np.where(
        (hours >= 6) & (hours < 18), 'Day', 'Night'
    )

    # State codes
    states = ['CA', 'TX', 'FL', 'NY', 'PA', 'IL', 'OH', 'GA', 'NC', 'MI']
    state = np.random.choice(states, n_samples)

    data = {
        'start_lat': latitudes,
        'start_lng': longitudes,
        'start_time': start_dates,
        'hour': hours,
        'severity': severity,
        'weather_condition': weather,
        'temperature_f': temperature,
        'wind_speed_mph': wind_speed,
        'bump': bump,
        'crossing': crossing,
        'junction': junction,
        'traffic_signal': traffic_signal,
        'sunrise_sunset': sunrise_sunset,
        'state': state
    }

    return pd.DataFrame(data)

# =============================================================================
# STEP 2: DATA PREPROCESSING & FEATURE ENGINEERING
# =============================================================================
def preprocess_accident_data(df):
    """Advanced preprocessing and feature engineering"""
    print("\n[2/6] DATA PREPROCESSING & FEATURE ENGINEERING...")

    df_clean = df.copy()

    # Convert datetime if string
    if df_clean['start_time'].dtype == 'object':
        df_clean['start_time'] = pd.to_datetime(df_clean['start_time'])

    # Extract temporal features
    df_clean['day_of_week'] = df_clean['start_time'].dt.day_name()
    df_clean['month'] = df_clean['start_time'].dt.month
    df_clean['year'] = df_clean['start_time'].dt.year
    df_clean['is_weekend'] = df_clean['start_time'].dt.dayofweek.isin([5, 6]).astype(int)

    # Time period categorization
    df_clean['time_period'] = pd.cut(
        df_clean['hour'],
        bins=[0, 6, 12, 18, 24],
        labels=['Night', 'Morning', 'Afternoon', 'Evening'],
        include_lowest=True
    )

    # Weather severity categorization
    severe_weather = ['Rain', 'Snow', 'Fog', 'Heavy Rain']
    df_clean['severe_weather'] = df_clean['weather_condition'].isin(severe_weather).astype(int)

    # Risk score calculation
    df_clean['risk_score'] = (
        df_clean['severity'] * 0.4 +
        df_clean['severe_weather'] * 0.3 +
        (df_clean['sunrise_sunset'] == 'Night').astype(int) * 0.2 +
        df_clean['junction'].astype(int) * 0.1
    )

    # Handle missing values
    numerical_cols = ['temperature_f', 'wind_speed_mph']
    for col in numerical_cols:
        df_clean[col].fillna(df_clean[col].median(), inplace=True)

    # Encode categorical variables
    le = LabelEncoder()
    categorical_cols = ['weather_condition', 'day_of_week', 'time_period', 'state']

    for col in categorical_cols:
        if col in df_clean.columns:
            df_clean[f'{col}_encoded'] = le.fit_transform(df_clean[col].astype(str))

    print(f"‚úì Preprocessing complete:")
    print(f"   ‚Ä¢ Temporal features: hour, day_of_week, month, year, time_period")
    print(f"   ‚Ä¢ Weather features: severe_weather indicator")
    print(f"   ‚Ä¢ Risk score: composite metric calculated")
    print(f"   ‚Ä¢ Encoded features: {len(categorical_cols)} categorical variables")

    return df_clean

# =============================================================================
# STEP 3: EXPLORATORY DATA ANALYSIS
# =============================================================================
def explore_accident_patterns(df):
    """Comprehensive EDA with professional visualizations"""
    print("\n[3/6] EXPLORATORY DATA ANALYSIS...")

    # Create comprehensive EDA dashboard
    create_eda_dashboard(df)

    # Print key statistics
    print(f"\nüìà Key Statistics:")
    print(f"   ‚Ä¢ Total Accidents: {len(df):,}")
    print(f"   ‚Ä¢ Average Severity: {df['severity'].mean():.2f}")
    print(f"   ‚Ä¢ Most Dangerous Hour: {df.groupby('hour')['severity'].mean().idxmax()}:00")
    print(f"   ‚Ä¢ Peak Accident Day: {df['day_of_week'].mode()[0]}")
    print(f"   ‚Ä¢ High-Risk States: {df.groupby('state').size().nlargest(3).index.tolist()}")

def create_eda_dashboard(df):
    """Create comprehensive EDA visualizations with clean layout"""

    fig = make_subplots(
        rows=3, cols=3,
        subplot_titles=(
            'Accidents by Hour of Day',
            'Severity Distribution',
            'Weather Conditions',
            'Accidents by Day of Week',
            'Temperature vs Severity',
            'Monthly Trend',
            'Road Features Impact',
            'Time Period Analysis',
            'State Distribution'
        ),
        specs=[
            [{"type": "scatter"}, {"type": "bar"}, {"type": "bar"}],
            [{"type": "bar"}, {"type": "scatter"}, {"type": "scatter"}],
            [{"type": "bar"}, {"type": "bar"}, {"type": "bar"}]
        ],
        vertical_spacing=0.15,
        horizontal_spacing=0.12
    )

    # 1. Accidents by Hour (Line with area fill)
    hourly = df.groupby('hour').size()
    fig.add_trace(
        go.Scatter(
            x=hourly.index,
            y=hourly.values,
            mode='lines+markers',
            name='Hourly Accidents',
            line=dict(color=COLORS['accent_blue'], width=3),
            fill='tozeroy',
            fillcolor='rgba(91, 155, 213, 0.3)',
            marker=dict(size=6),
            hovertemplate='Hour: %{x}<br>Accidents: %{y:,}<extra></extra>'
        ),
        row=1, col=1
    )

    # 2. Severity Distribution
    severity_counts = df['severity'].value_counts().sort_index()
    fig.add_trace(
        go.Bar(
            x=[f"Level {s}" for s in severity_counts.index],
            y=severity_counts.values,
            marker=dict(
                color=[COLORS['accent_teal'], COLORS['accent_cyan'],
                       COLORS['accent_steel'], COLORS['accent_slate']],
                line=dict(color=COLORS['border'], width=1)
            ),
            text=[f"{v:,}" for v in severity_counts.values],
            textposition='outside',
            textfont=dict(size=10, color=COLORS['text_primary']),
            showlegend=False,
            hovertemplate='%{x}<br>Count: %{y:,}<extra></extra>'
        ),
        row=1, col=2
    )

    # 3. Weather Conditions (Top 7)
    weather_counts = df['weather_condition'].value_counts().head(7)
    fig.add_trace(
        go.Bar(
            x=weather_counts.index,
            y=weather_counts.values,
            marker=dict(color=COLORS['accent_cyan'], line=dict(color=COLORS['border'], width=1)),
            text=[f"{v:,}" for v in weather_counts.values],
            textposition='outside',
            textfont=dict(size=9, color=COLORS['text_primary']),
            showlegend=False,
            hovertemplate='%{x}<br>Accidents: %{y:,}<extra></extra>'
        ),
        row=1, col=3
    )

    # 4. Day of Week
    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    day_counts = df['day_of_week'].value_counts().reindex(day_order)

    fig.add_trace(
        go.Bar(
            x=day_counts.index,
            y=day_counts.values,
            marker=dict(color=COLORS['accent_teal'], line=dict(color=COLORS['border'], width=1)),
            text=[f"{v:,}" for v in day_counts.values],
            textposition='outside',
            textfont=dict(size=9, color=COLORS['text_primary']),
            showlegend=False,
            hovertemplate='%{x}<br>Accidents: %{y:,}<extra></extra>'
        ),
        row=2, col=1
    )

    # 5. Temperature vs Severity (Sample for performance)
    sample_df = df.sample(min(2000, len(df)))
    fig.add_trace(
        go.Scatter(
            x=sample_df['temperature_f'],
            y=sample_df['severity'],
            mode='markers',
            marker=dict(
                color=sample_df['severity'],
                colorscale=[[0, COLORS['accent_teal']], [1, COLORS['accent_slate']]],
                size=4,
                opacity=0.5,
                line=dict(width=0)
            ),
            showlegend=False,
            hovertemplate='Temp: %{x:.1f}¬∞F<br>Severity: %{y}<extra></extra>'
        ),
        row=2, col=2
    )

    # 6. Monthly Trend
    monthly = df.groupby('month').size()
    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

    fig.add_trace(
        go.Scatter(
            x=[month_names[m-1] for m in monthly.index],
            y=monthly.values,
            mode='lines+markers',
            line=dict(color=COLORS['accent_steel'], width=3),
            marker=dict(size=8),
            fill='tozeroy',
            fillcolor='rgba(100, 116, 139, 0.3)',
            showlegend=False,
            hovertemplate='%{x}<br>Accidents: %{y:,}<extra></extra>'
        ),
        row=2, col=3
    )

    # 7. Road Features Impact
    features = ['bump', 'crossing', 'junction', 'traffic_signal']
    feature_counts = [df[f].sum() for f in features]
    feature_labels = ['Bump', 'Crossing', 'Junction', 'Signal']

    fig.add_trace(
        go.Bar(
            x=feature_labels,
            y=feature_counts,
            marker=dict(color=COLORS['accent_slate'], line=dict(color=COLORS['border'], width=1)),
            text=[f"{v:,}" for v in feature_counts],
            textposition='outside',
            textfont=dict(size=10, color=COLORS['text_primary']),
            showlegend=False,
            hovertemplate='%{x}<br>Count: %{y:,}<extra></extra>'
        ),
        row=3, col=1
    )

    # 8. Time Period Analysis
    if 'time_period' in df.columns:
        period_counts = df['time_period'].value_counts().reindex(['Morning', 'Afternoon', 'Evening', 'Night'])
        fig.add_trace(
            go.Bar(
                x=period_counts.index,
                y=period_counts.values,
                marker=dict(color=COLORS['accent_cyan'], line=dict(color=COLORS['border'], width=1)),
                text=[f"{v:,}" for v in period_counts.values],
                textposition='outside',
                textfont=dict(size=10, color=COLORS['text_primary']),
                showlegend=False,
                hovertemplate='%{x}<br>Accidents: %{y:,}<extra></extra>'
            ),
            row=3, col=2
        )

    # 9. State Distribution (Top 10)
    state_counts = df['state'].value_counts().head(10)
    fig.add_trace(
        go.Bar(
            x=state_counts.index,
            y=state_counts.values,
            marker=dict(color=COLORS['accent_steel'], line=dict(color=COLORS['border'], width=1)),
            text=[f"{v:,}" for v in state_counts.values],
            textposition='outside',
            textfont=dict(size=9, color=COLORS['text_primary']),
            showlegend=False,
            hovertemplate='%{x}<br>Accidents: %{y:,}<extra></extra>'
        ),
        row=3, col=3
    )

    # Update layout
    fig.update_layout(
        title={
            'text': '<b>EXPLORATORY DATA ANALYSIS</b><br><sub style="font-size:12px">Traffic Accident Patterns & Distribution</sub>',
            'x': 0.5,
            'xanchor': 'center',
            'y': 0.98,
            'font': {'size': 22, 'color': COLORS['text_primary'], 'family': 'Arial'}
        },
        paper_bgcolor=COLORS['bg_primary'],
        plot_bgcolor=COLORS['bg_secondary'],
        font=dict(color=COLORS['text_secondary'], family='Arial', size=10),
        height=1400,
        width=1800,
        showlegend=False,
        margin=dict(t=100, b=100, l=70, r=70)
    )

    # Update axes
    fig.update_xaxes(
        showgrid=True, gridcolor=COLORS['grid'], gridwidth=0.5,
        showline=True, linecolor=COLORS['border'], linewidth=1,
        tickfont=dict(size=9, color=COLORS['text_secondary']),
        title_font=dict(size=10, color=COLORS['text_primary'])
    )
    fig.update_yaxes(
        showgrid=True, gridcolor=COLORS['grid'], gridwidth=0.5,
        showline=True, linecolor=COLORS['border'], linewidth=1,
        tickfont=dict(size=9, color=COLORS['text_secondary']),
        title_font=dict(size=10, color=COLORS['text_primary'])
    )

    # Update subplot titles
    for i, annotation in enumerate(fig['layout']['annotations']):
        if i < 9:
            annotation['font'] = dict(size=11, color=COLORS['accent_cyan'], family='Arial')
            annotation['y'] = annotation['y'] + 0.02

    # Axis labels
    fig.update_xaxes(title_text="Hour of Day", row=1, col=1)
    fig.update_yaxes(title_text="Accident Count", row=1, col=1)

    fig.update_xaxes(title_text="Severity Level", row=1, col=2)
    fig.update_yaxes(title_text="Count", row=1, col=2)

    fig.update_xaxes(title_text="Weather", row=1, col=3)
    fig.update_yaxes(title_text="Accidents", row=1, col=3)

    fig.update_xaxes(title_text="Day", row=2, col=1)
    fig.update_yaxes(title_text="Accidents", row=2, col=1)

    fig.update_xaxes(title_text="Temperature (¬∞F)", row=2, col=2)
    fig.update_yaxes(title_text="Severity", row=2, col=2)

    fig.update_xaxes(title_text="Month", row=2, col=3)
    fig.update_yaxes(title_text="Accidents", row=2, col=3)

    fig.update_xaxes(title_text="Road Feature", row=3, col=1)
    fig.update_yaxes(title_text="Count", row=3, col=1)

    fig.update_xaxes(title_text="Time Period", row=3, col=2)
    fig.update_yaxes(title_text="Accidents", row=3, col=2)

    fig.update_xaxes(title_text="State", row=3, col=3)
    fig.update_yaxes(title_text="Accidents", row=3, col=3)

    fig.show()
    print("‚úì EDA Dashboard created")

# =============================================================================
# STEP 4: ADVANCED ANALYTICS - CLUSTERING & MODELING
# =============================================================================
def perform_advanced_analytics(df):
    """KMeans clustering and statistical modeling"""
    print("\n[4/6] ADVANCED ANALYTICS...")

    # Hotspot clustering
    clusters_df, n_clusters = identify_accident_hotspots(df)

    # Statistical modeling
    model_results = build_severity_model(df)

    return clusters_df, model_results

def identify_accident_hotspots(df):
    """KMeans clustering to identify accident hotspots"""
    print("‚öô Identifying accident hotspots using KMeans clustering...")

    # Prepare coordinates
    coords = df[['start_lng', 'start_lat']].dropna()

    # Optimal clusters using elbow method (simplified to 5 for efficiency)
    n_clusters = 5

    # Perform clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    coords['cluster'] = kmeans.fit_predict(coords)

    # Calculate silhouette score
    silhouette = silhouette_score(coords[['start_lng', 'start_lat']], coords['cluster'])

    print(f"‚úì Hotspot clustering complete:")
    print(f"   ‚Ä¢ Clusters identified: {n_clusters}")
    print(f"   ‚Ä¢ Silhouette score: {silhouette:.3f}")
    print(f"   ‚Ä¢ Cluster sizes: {coords['cluster'].value_counts().sort_index().values}")

    return coords, n_clusters

def build_severity_model(df):
    """Build GLM to identify factors contributing to accident severity"""
    print("‚öô Building statistical model for severity factors...")

    # Prepare features
    model_df = df[[
        'temperature_f', 'wind_speed_mph', 'hour',
        'severe_weather', 'junction', 'traffic_signal'
    ]].copy()

    # Remove any remaining NaN
    model_df = model_df.dropna()
    y = df.loc[model_df.index, 'severity']

    # Add constant
    X = sm.add_constant(model_df)

    # Fit Poisson GLM
    try:
        glm_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()

        print(f"‚úì Statistical model built:")
        print(f"   ‚Ä¢ Model type: Poisson GLM")
        print(f"   ‚Ä¢ Features: {len(model_df.columns)}")
        print(f"   ‚Ä¢ AIC: {glm_model.aic:.2f}")

        # Extract significant factors
        sig_factors = glm_model.pvalues[glm_model.pvalues < 0.05].index.tolist()
        print(f"   ‚Ä¢ Significant factors: {', '.join(sig_factors[1:])}")  # Skip const

        return glm_model
    except Exception as e:
        print(f"‚ö† Model fitting warning: {e}")
        return None

# =============================================================================
# STEP 5: ADVANCED VISUALIZATIONS
# =============================================================================
def create_analytics_dashboard(df, clusters_df, model_results):
    """Create comprehensive analytics dashboard"""
    print("\n[5/6] GENERATING ADVANCED VISUALIZATIONS...")

    fig = make_subplots(
        rows=3, cols=3,
        subplot_titles=(
            'Accident Hotspots Map',
            'Cluster Distribution',
            'Risk Score Distribution',
            'Severity by Weather',
            'Hour vs Severity Heatmap',
            'Feature Correlation',
            'Day vs Night Comparison',
            'Severity Factors',
            'Trend Analysis'
        ),
        specs=[
            [{"type": "scattergeo"}, {"type": "bar"}, {"type": "violin"}],
            [{"type": "box"}, {"type": "heatmap"}, {"type": "heatmap"}],
            [{"type": "bar"}, {"type": "bar"}, {"type": "scatter"}]
        ],
        vertical_spacing=0.14,
        horizontal_spacing=0.12
    )

    # 1. Geographic Hotspots (sample for performance)
    sample_clusters = clusters_df.sample(min(5000, len(clusters_df)))
    fig.add_trace(
        go.Scattergeo(
            lon=sample_clusters['start_lng'],
            lat=sample_clusters['start_lat'],
            mode='markers',
            marker=dict(
                size=3,
                color=sample_clusters['cluster'],
                colorscale=[[0, COLORS['accent_teal']], [0.5, COLORS['accent_cyan']], [1, COLORS['accent_slate']]],
                opacity=0.6,
                line=dict(width=0)
            ),
            showlegend=False,
            hovertemplate='Lat: %{lat:.2f}<br>Lon: %{lon:.2f}<br>Cluster: %{marker.color}<extra></extra>'
        ),
        row=1, col=1
    )

    # 2. Cluster Size Distribution
    cluster_sizes = clusters_df['cluster'].value_counts().sort_index()
    fig.add_trace(
        go.Bar(
            x=[f"Cluster {i}" for i in cluster_sizes.index],
            y=cluster_sizes.values,
            marker=dict(color=COLORS['accent_blue'], line=dict(color=COLORS['border'], width=1)),
            text=[f"{v:,}" for v in cluster_sizes.values],
            textposition='outside',
            textfont=dict(size=10, color=COLORS['text_primary']),
            showlegend=False
        ),
        row=1, col=2
    )

    # 3. Risk Score Distribution
    if 'risk_score' in df.columns:
        for sev in sorted(df['severity'].unique()):
            fig.add_trace(
                go.Violin(
                    y=df[df['severity'] == sev]['risk_score'],
                    name=f"Severity {sev}",
                    box_visible=True,
                    meanline_visible=True,
                    marker=dict(color=COLORS['accent_cyan'])
                ),
                row=1, col=3
            )

    # 4. Severity by Weather (Box plot)
    weather_top = df['weather_condition'].value_counts().head(5).index
    for weather in weather_top:
        weather_data = df[df['weather_condition'] == weather]
        fig.add_trace(
            go.Box(
                y=weather_data['severity'],
                name=weather,
                marker=dict(color=COLORS['accent_teal']),
                showlegend=False
            ),
            row=2, col=1
        )

    # 5. Hour vs Severity Heatmap
    hour_severity = pd.crosstab(df['hour'], df['severity'])
    fig.add_trace(
        go.Heatmap(
            z=hour_severity.values,
            x=[f"Sev {i}" for i in hour_severity.columns],
            y=hour_severity.index,
            colorscale=[[0, COLORS['surface']], [1, COLORS['accent_blue']]],
            showscale=False,
            hovertemplate='Hour: %{y}<br>Severity: %{x}<br>Count: %{z}<extra></extra>'
        ),
        row=2, col=2
    )

    # 6. Feature Correlation
    corr_features = ['severity', 'temperature_f', 'wind_speed_mph', 'hour']
    if 'risk_score' in df.columns:
        corr_features.append('risk_score')

    corr_matrix = df[corr_features].corr()
    fig.add_trace(
        go.Heatmap(
            z=corr_matrix.values,
            x=corr_matrix.columns,
            y=corr_matrix.columns,
            colorscale=[[0, COLORS['accent_teal']], [0.5, COLORS['surface_light']], [1, COLORS['accent_slate']]],
            text=np.round(corr_matrix.values, 2),
            texttemplate='%{text}',
            textfont=dict(size=9),
            showscale=False
        ),
        row=2, col=3
    )

    # 7. Day vs Night
    day_night = df.groupby('sunrise_sunset')['severity'].mean()
    fig.add_trace(
        go.Bar(
            x=day_night.index,
            y=day_night.values,
            marker=dict(color=[COLORS['warning'], COLORS['accent_steel']],
                       line=dict(color=COLORS['border'], width=1)),
            text=[f"{v:.2f}" for v in day_night.values],
            textposition='outside',
            textfont=dict(size=11, color=COLORS['text_primary']),
            showlegend=False
        ),
        row=3, col=1
    )

    # 8. Top Severity Factors
    if model_results is not None:
        # Extract coefficients
        coef_df = pd.DataFrame({
            'Feature': model_results.params.index[1:6],  # Top 5 excluding const
            'Coefficient': model_results.params.values[1:6]
        }).sort_values('Coefficient', ascending=True)

        fig.add_trace(
            go.Bar(
                y=coef_df['Feature'],
                x=coef_df['Coefficient'],
                orientation='h',
                marker=dict(color=COLORS['accent_cyan'], line=dict(color=COLORS['border'], width=1)),
                text=[f"{v:.3f}" for v in coef_df['Coefficient']],
                textposition='outside',
                textfont=dict(size=9, color=COLORS['text_primary']),
                showlegend=False
            ),
            row=3, col=2
        )

    # 9. Yearly Trend
    if 'year' in df.columns:
        yearly_trend = df.groupby('year').size()
        fig.add_trace(
            go.Scatter(
                x=yearly_trend.index,
                y=yearly_trend.values,
                mode='lines+markers',
                line=dict(color=COLORS['accent_steel'], width=3),
                marker=dict(size=10),
                fill='tozeroy',
                fillcolor='rgba(100, 116, 139, 0.3)',
                showlegend=False,
                hovertemplate='Year: %{x}<br>Accidents: %{y:,}<extra></extra>'
            ),
            row=3, col=3
        )

    # Update layout
    fig.update_layout(
        title={
            'text': '<b>ADVANCED ANALYTICS DASHBOARD</b><br><sub style="font-size:12px">Hotspots, Patterns & Statistical Insights</sub>',
            'x': 0.5,
            'xanchor': 'center',
            'y': 0.98,
            'font': {'size': 22, 'color': COLORS['text_primary'], 'family': 'Arial'}
        },
        paper_bgcolor=COLORS['bg_primary'],
        plot_bgcolor=COLORS['bg_secondary'],
        font=dict(color=COLORS['text_secondary'], family='Arial', size=10),
        height=1500,
        width=1800,
        showlegend=True,
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=-0.08,
            xanchor="center",
            x=0.5,
            font=dict(size=9)
        ),
        margin=dict(t=100, b=100, l=70, r=70)
    )

    # Update geo layout
    fig.update_geos(
        scope='usa',
        projection_type='albers usa',
        showland=True,
        landcolor=COLORS['surface'],
        coastlinecolor=COLORS['border'],
        showlakes=False,
        bgcolor=COLORS['bg_secondary']
    )

    # Update axes
    fig.update_xaxes(
        showgrid=True, gridcolor=COLORS['grid'], gridwidth=0.5,
        showline=True, linecolor=COLORS['border'], linewidth=1,
        tickfont=dict(size=9, color=COLORS['text_secondary']),
        title_font=dict(size=10, color=COLORS['text_primary'])
    )
    fig.update_yaxes(
        showgrid=True, gridcolor=COLORS['grid'], gridwidth=0.5,
        showline=True, linecolor=COLORS['border'], linewidth=1,
        tickfont=dict(size=9, color=COLORS['text_secondary']),
        title_font=dict(size=10, color=COLORS['text_primary'])
    )

    # Update subplot titles
    for i, annotation in enumerate(fig['layout']['annotations']):
        if i < 9:
            annotation['font'] = dict(size=11, color=COLORS['accent_cyan'], family='Arial')
            annotation['y'] = annotation['y'] + 0.025

    # Axis labels
    fig.update_xaxes(title_text="Cluster ID", row=1, col=2)
    fig.update_yaxes(title_text="Accident Count", row=1, col=2)

    fig.update_yaxes(title_text="Risk Score", row=1, col=3)

    fig.update_yaxes(title_text="Severity", row=2, col=1)

    fig.update_xaxes(title_text="Severity Level", row=2, col=2)
    fig.update_yaxes(title_text="Hour of Day", row=2, col=2)

    fig.update_xaxes(title_text="Time of Day", row=3, col=1)
    fig.update_yaxes(title_text="Avg Severity", row=3, col=1)

    fig.update_xaxes(title_text="Coefficient", row=3, col=2)
    fig.update_yaxes(title_text="Feature", row=3, col=2)

    fig.update_xaxes(title_text="Year", row=3, col=3)
    fig.update_yaxes(title_text="Accidents", row=3, col=3)

    fig.show()
    print("‚úì Analytics Dashboard created")

# =============================================================================
# STEP 6: INSIGHTS & RECOMMENDATIONS
# =============================================================================
def generate_insights_report(df, clusters_df, model_results):
    """Generate comprehensive insights and recommendations"""
    print("\n[6/6] GENERATING INSIGHTS & RECOMMENDATIONS...")

    print("\n" + "="*80)
    print("KEY INSIGHTS & FINDINGS")
    print("="*80)

    # 1. Temporal Patterns
    peak_hour = df.groupby('hour')['severity'].mean().idxmax()
    peak_day = df['day_of_week'].value_counts().idxmax()

    print(f"\nüïê TEMPORAL PATTERNS:")
    print(f"   ‚Ä¢ Peak Accident Hour: {peak_hour}:00 ({df[df['hour']==peak_hour].shape[0]:,} accidents)")
    print(f"   ‚Ä¢ Most Dangerous Day: {peak_day}")
    print(f"   ‚Ä¢ Weekend vs Weekday: {df['is_weekend'].value_counts().to_dict()}")

    # 2. Weather Impact
    severe_weather_accidents = df[df['severe_weather'] == 1]
    print(f"\nüåßÔ∏è WEATHER IMPACT:")
    print(f"   ‚Ä¢ Severe Weather Accidents: {len(severe_weather_accidents):,} ({len(severe_weather_accidents)/len(df)*100:.1f}%)")
    print(f"   ‚Ä¢ Avg Severity (Normal): {df[df['severe_weather']==0]['severity'].mean():.2f}")
    print(f"   ‚Ä¢ Avg Severity (Severe): {severe_weather_accidents['severity'].mean():.2f}")

    # 3. Geographic Hotspots
    top_states = df['state'].value_counts().head(5)
    print(f"\nüìç GEOGRAPHIC HOTSPOTS:")
    print(f"   ‚Ä¢ Total Clusters Identified: {clusters_df['cluster'].nunique()}")
    print(f"   ‚Ä¢ Top 5 States:")
    for state, count in top_states.items():
        print(f"      - {state}: {count:,} accidents")

    # 4. Risk Factors
    if 'risk_score' in df.columns:
        high_risk = df[df['risk_score'] > df['risk_score'].quantile(0.75)]
        print(f"\n‚ö†Ô∏è RISK FACTORS:")
        print(f"   ‚Ä¢ High-Risk Accidents (Top 25%): {len(high_risk):,}")
        print(f"   ‚Ä¢ Avg Risk Score: {df['risk_score'].mean():.2f}")
        print(f"   ‚Ä¢ High-Risk Characteristics:")
        print(f"      - Junction Present: {high_risk['junction'].mean()*100:.1f}%")
        print(f"      - Night Time: {(high_risk['sunrise_sunset']=='Night').mean()*100:.1f}%")
        print(f"      - Severe Weather: {high_risk['severe_weather'].mean()*100:.1f}%")

    # 5. Severity Analysis
    print(f"\nüìä SEVERITY DISTRIBUTION:")
    for sev in sorted(df['severity'].unique()):
        count = (df['severity'] == sev).sum()
        pct = count / len(df) * 100
        print(f"   ‚Ä¢ Level {sev}: {count:,} accidents ({pct:.1f}%)")

    # 6. Road Features
    print(f"\nüõ£Ô∏è ROAD FEATURE ANALYSIS:")
    print(f"   ‚Ä¢ Accidents at Junctions: {df['junction'].sum():,} ({df['junction'].mean()*100:.1f}%)")
    print(f"   ‚Ä¢ Accidents at Crossings: {df['crossing'].sum():,} ({df['crossing'].mean()*100:.1f}%)")
    print(f"   ‚Ä¢ Accidents with Traffic Signals: {df['traffic_signal'].sum():,} ({df['traffic_signal'].mean()*100:.1f}%)")
    print(f"   ‚Ä¢ Accidents with Bumps: {df['bump'].sum():,} ({df['bump'].mean()*100:.1f}%)")

    print("\n" + "="*80)
    print("RECOMMENDATIONS")
    print("="*80)

    print(f"\n‚úì IMMEDIATE ACTIONS:")
    print(f"   1. Increase traffic enforcement during {peak_hour}:00 hour")
    print(f"   2. Deploy additional resources in top 5 accident states")
    print(f"   3. Enhance road safety measures at identified hotspot clusters")
    print(f"   4. Improve weather warning systems for severe conditions")

    print(f"\n‚úì STRATEGIC INITIATIVES:")
    print(f"   1. Infrastructure improvements at high-risk junctions and crossings")
    print(f"   2. Enhanced lighting systems for night-time accident reduction")
    print(f"   3. Weather-adaptive traffic management systems")
    print(f"   4. Public awareness campaigns for peak accident periods")

    print(f"\n‚úì POLICY RECOMMENDATIONS:")
    print(f"   1. Implement dynamic speed limits based on weather conditions")
    print(f"   2. Mandatory advanced driver training for high-risk conditions")
    print(f"   3. Enhanced road maintenance schedules for accident hotspots")
    print(f"   4. Real-time accident prediction and prevention systems")

    print("\n" + "="*80)
    print("‚úì Analysis Complete! Check visualizations for detailed patterns.")
    print("="*80)

# =============================================================================
# MAIN EXECUTION PIPELINE
# =============================================================================
def main():
    """Execute complete traffic accident analysis pipeline"""

    # Step 1: Load Data
    df = load_accident_data()

    # Step 2: Preprocess
    df_clean = preprocess_accident_data(df)

    # Step 3: Explore
    explore_accident_patterns(df_clean)

    # Step 4: Advanced Analytics
    clusters_df, model_results = perform_advanced_analytics(df_clean)

    # Step 5: Visualize Advanced Analytics
    create_analytics_dashboard(df_clean, clusters_df, model_results)

    # Step 6: Generate Insights
    generate_insights_report(df_clean, clusters_df, model_results)

# Run the pipeline
if __name__ == "__main__":
    main()